{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SVM & Naive Bayes | Assignment"
      ],
      "metadata": {
        "id": "l_y-J1Xh3CHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is a Support Vector Machine (SVM), and how does it work?**\n"
      ],
      "metadata": {
        "id": "KmHjPJ7N3_ZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans-**\n",
        "\n",
        "Support Vector Machine (SVM):\n",
        "\n",
        " - A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification (mainly) and regression problems. It is particularly powerful in high-dimensional spaces and works well when the number of dimensions exceeds the number of samples.\n",
        "\n",
        "How SVM Works:\n",
        "\n",
        " - Separating Classes with a Hyperplane\n",
        "\n",
        " - SVM tries to find the best decision boundary (called a hyperplane) that separates data points of different classes.\n",
        "\n",
        " - For example, in 2D space, this boundary is a line; in 3D, it‚Äôs a plane; in higher dimensions, it‚Äôs a hyperplane.\n",
        "\n",
        "Maximizing the Margin\n",
        "\n",
        " - SVM doesn‚Äôt just find any boundary ‚Äî it finds the one that maximizes the margin, i.e., the distance between the hyperplane and the nearest data points from each class.\n",
        "\n",
        " - These nearest points are called support vectors (hence the name).\n",
        "\n",
        "Handling Non-Linearly Separable Data\n",
        "\n",
        " - When data is not linearly separable, SVM uses the kernel trick to transform data into a higher-dimensional space where it becomes separable.\n",
        "\n",
        " - Common kernels:\n",
        "\n",
        " - Linear kernel (for linearly separable data)\n",
        "\n",
        " - Polynomial kernel\n",
        "\n",
        " - Radial Basis Function (RBF) kernel (very popular)\n",
        "\n",
        "Soft Margin (for noisy data)\n",
        "\n",
        " - In real-world datasets, perfect separation is not always possible.\n",
        "\n",
        " - SVM allows some misclassification using a regularization parameter (C) that controls the trade-off between maximizing the margin and minimizing classification errors."
      ],
      "metadata": {
        "id": "pCQd4vJ-3F6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Explain the difference between Hard Margin and Soft Margin SVM.**"
      ],
      "metadata": {
        "id": "neS5WZCv38nh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans-**\n",
        "\n",
        "**1. Hard Margin SVM**\n",
        "\n",
        "Definition:\n",
        " - Hard Margin SVM requires that all training data points be classified perfectly by the hyperplane, with no misclassifications.\n",
        "\n",
        "Conditions:\n",
        "\n",
        " - Works only if the data is linearly separable.\n",
        "\n",
        " - All data points must lie outside the margin.\n",
        "\n",
        "Advantages:\n",
        "\n",
        " - Produces a very strict boundary with maximum margin.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        " - Not suitable for noisy datasets (outliers can drastically affect the boundary).\n",
        "\n",
        " - Rarely works in real-world scenarios since perfect separation is uncommon.\n",
        "\n",
        "**2. Soft Margin SVM**\n",
        "\n",
        "Definition:\n",
        " - Soft Margin SVM allows some misclassifications or violations of the margin, controlled by a parameter C.\n",
        "\n",
        "Conditions:\n",
        "\n",
        " - Works well for non-linearly separable data.\n",
        "\n",
        " - Strikes a balance between maximizing margin and minimizing classification error.\n",
        "\n",
        "Advantages:\n",
        "\n",
        " - More flexible and robust against outliers and noise.\n",
        "\n",
        " - Works better in real-world datasets.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        " - May not achieve perfect classification on training data."
      ],
      "metadata": {
        "id": "fIVQjgV64SSv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.**\n"
      ],
      "metadata": {
        "id": "DtfB51WZ5MQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans-**\n",
        "\n",
        "**Kernel Trick in SVM**\n",
        "\n",
        " - The Kernel Trick is a mathematical technique that allows SVM to solve problems where the data is not linearly separable.\n",
        "\n",
        " - Instead of working in the original input space, it maps the data into a higher-dimensional space where it becomes separable by a hyperplane.\n",
        "\n",
        " - This makes computation efficient even when the feature space is very high-dimensional.\n",
        "\n",
        "Example Kernel: Radial Basis Function (RBF) Kernel\n",
        "\n",
        "Formula:\n",
        "\n",
        "ùêæ\n",
        "(\n",
        "ùë•\n",
        ",\n",
        "ùë•\n",
        "‚Ä≤\n",
        ")\n",
        "=\n",
        "exp\n",
        "‚Å°\n",
        "(\n",
        "‚àí\n",
        "ùõæ\n",
        "‚à•\n",
        "ùë•\n",
        "‚àí\n",
        "ùë•\n",
        "‚Ä≤\n",
        "‚à•\n",
        "2\n",
        ")\n",
        "K(x,x\n",
        "‚Ä≤\n",
        ")=exp(‚àíŒ≥‚à•x‚àíx\n",
        "‚Ä≤\n",
        "‚à•\n",
        "2\n",
        ")\n",
        "\n",
        "where\n",
        "\n",
        "ùë•\n",
        ",\n",
        "ùë•\n",
        "‚Ä≤\n",
        "x,x\n",
        "‚Ä≤\n",
        " = input vectors\n",
        "\n",
        "ùõæ\n",
        "Œ≥ = parameter controlling the influence of each data point\n",
        "\n",
        "**Use Case:**\n",
        "\n",
        " - RBF kernel is very powerful for non-linear decision boundaries.\n",
        "\n",
        "Example:\n",
        "\n",
        " -  In image classification, spam detection, or handwriting recognition, the data is not linearly separable. RBF kernel maps data into a higher dimension where a hyperplane can separate classes effectively.\n",
        "\n"
      ],
      "metadata": {
        "id": "HPyt0WCq5qSE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What is a Na√Øve Bayes Classifier, and why is it called ‚Äúna√Øve‚Äù?**"
      ],
      "metadata": {
        "id": "3WloVehY6ipz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans-**\n",
        "\n",
        "**Na√Øve Bayes Classifier**\n",
        "\n",
        "**Definition:**\n",
        "\n",
        " - Na√Øve Bayes is a probabilistic machine learning algorithm based on Bayes‚Äô Theorem.\n",
        " - It is mainly used for classification tasks (like spam detection, sentiment analysis, text categorization).\n",
        "\n",
        "Core Idea:\n",
        "\n",
        " - It predicts the class of a sample based on the probability of features belonging to each class.\n",
        "\n",
        "Why is it called ‚ÄúNa√Øve‚Äù?\n",
        "\n",
        "   - Because it makes a na√Øve assumption:\n",
        "\n",
        " - All features are conditionally independent given the class.\n",
        "\n",
        " -  In reality, features are often correlated (e.g., in text, the words ‚Äúmoney‚Äù and ‚Äúbank‚Äù often occur together).\n",
        "But Na√Øve Bayes still works surprisingly well despite this unrealistic assumption."
      ],
      "metadata": {
        "id": "hOI98he36zCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: Describe the Gaussian, Multinomial, and Bernoulli Na√Øve Bayes variants. When would you use each one?**\n",
        "\n",
        "Dataset Info:\n",
        "‚óè You can use any suitable datasets like Iris, Breast Cancer, or Wine from\n",
        "sklearn.datasets or a CSV file you have.\n",
        "\n"
      ],
      "metadata": {
        "id": "oefFfBkY7h8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans- **\n",
        "\n",
        "**1. Gaussian Na√Øve Bayes**\n",
        "\n",
        "Assumption:\n",
        " - Features follow a continuous Gaussian (Normal) distribution.\n",
        "\n",
        "Use Case:\n",
        "\n",
        " - When features are continuous (not categorical).\n",
        "\n",
        "Example:\n",
        "\n",
        " - Predicting whether a patient has a disease based on height, weight, blood pressure, and cholesterol levels (all continuous values).\n",
        "\n",
        "**2. Multinomial Na√Øve Bayes**\n",
        "\n",
        "Assumption:\n",
        " - Features represent counts or frequencies.\n",
        "\n",
        " - Example: number of times a word appears in a document.\n",
        "\n",
        "Use Case:\n",
        "\n",
        " - Best for text classification problems (spam detection, sentiment analysis, document categorization).\n",
        "\n",
        "Example:\n",
        "\n",
        " - Classifying news articles into topics based on word frequencies.\n",
        "\n",
        "**3. Bernoulli Na√Øve Bayes**\n",
        "\n",
        "Assumption:\n",
        " - Features are binary (0 or 1) ‚Äî presence/absence indicators.\n",
        "\n",
        "Use Case:\n",
        "\n",
        " - When data is boolean in nature.\n",
        "\n",
        "Example:\n",
        "\n",
        " - Text classification based on whether a word appears or not (not how many times).\n",
        "\n",
        " - Document classification using \"word present = 1, word absent = 0\"."
      ],
      "metadata": {
        "id": "ri6WLThy7vek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Info:\n",
        "‚óè You can use any suitable datasets like Iris, Breast Cancer, or Wine from\n",
        "sklearn.datasets or a CSV file you have.**\n"
      ],
      "metadata": {
        "id": "-aOodOIe8fx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: Write a Python program to:**\n",
        "    \n",
        "      ‚óè Load the Iris dataset\n",
        "      ‚óè Train an SVM Classifier with a linear kernel\n",
        "      ‚óè Print the model's accuracy and support vectors.\n",
        "      (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "WJ9_-79w949T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBKK9SZz270z",
        "outputId": "53e70351-d69e-4f5d-e8cc-3108e190eab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Classifier (Linear Kernel) Accuracy: 1.0\n",
            "\n",
            "Support Vectors:\n",
            " [[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n",
            "\n",
            "Number of Support Vectors for each class: [ 3 11 10]\n"
          ]
        }
      ],
      "source": [
        "# **Ans-** SVM Classifier on Iris Dataset (Linear Kernel)\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train an SVM Classifier with linear kernel\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict on test data\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# 5. Print accuracy and support vectors\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"SVM Classifier (Linear Kernel) Accuracy:\", accuracy)\n",
        "print(\"\\nSupport Vectors:\\n\", svm_model.support_vectors_)\n",
        "print(\"\\nNumber of Support Vectors for each class:\", svm_model.n_support_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Write a Python program to:**\n",
        "\n",
        "    ‚óè Load the Breast Cancer dataset\n",
        "    ‚óè Train a Gaussian Na√Øve Bayes model\n",
        "    ‚óè Print its classification report including precision, recall, and F1-score.\n",
        "    (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "8XwFcQ2r-Xw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# **Ans-** Gaussian Na√Øve Bayes on Breast Cancer Dataset\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train Gaussian Na√Øve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict on test data\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# 5. Print classification report\n",
        "print(\"Classification Report for Gaussian Na√Øve Bayes:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8HhMWSf-Tod",
        "outputId": "220f3a0b-c2cb-41c4-963b-ee4b88fc8374"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for Gaussian Na√Øve Bayes:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.93      0.90      0.92        63\n",
            "      benign       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Write a Python program to:**\n",
        "\n",
        "    ‚óè Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "    C and gamma.\n",
        "    ‚óè Print the best hyperparameters and accuracy.\n",
        "    (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "oy6Shv94-uVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# **Ans-** SVM with GridSearchCV on Wine Dataset\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Define SVM model and parameter grid\n",
        "svm_model = SVC(kernel='rbf')\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1]\n",
        "}\n",
        "\n",
        "# 4. GridSearchCV to tune hyperparameters\n",
        "grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 5. Best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# 6. Print results\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wpUzWUC-pU5",
        "outputId": "83e20a15-29ae-493d-e2a1-f6badbc99576"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 10, 'gamma': 0.001}\n",
            "Best Cross-Validation Accuracy: 0.6946666666666667\n",
            "Test Accuracy: 0.7777777777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Write a Python program to:**\n",
        "\n",
        "    ‚óè Train a Na√Øve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "    sklearn.datasets.fetch_20newsgroups).\n",
        "    ‚óè Print the model's ROC-AUC score for its predictions.\n",
        "    (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "993MW9xH--Mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# **Ans**:- Na√Øve Bayes on Text Dataset with ROC-AUC\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load subset of 20 Newsgroups dataset (binary classification for ROC-AUC)\n",
        "categories = ['sci.space', 'rec.sport.baseball']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "\n",
        "X, y = newsgroups.data, newsgroups.target  # y = 0/1\n",
        "\n",
        "# 2. Convert text to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# 3. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Train Na√Øve Bayes classifier\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predict probabilities for ROC-AUC\n",
        "y_probs = nb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# 6. Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "\n",
        "print(\"Na√Øve Bayes ROC-AUC Score:\", roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yBRvhBs_KZd",
        "outputId": "3724f01a-2294-4aae-f4e3-32c5afb4b95c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Na√Øve Bayes ROC-AUC Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Imagine you‚Äôre working as a data scientist for a company that handles email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:**\n",
        "\n",
        "    ‚óè Text with diverse vocabulary\n",
        "    ‚óè Potential class imbalance (far more legitimate emails than spam)\n",
        "    ‚óè Some incomplete or missing data\n",
        "\n",
        "Explain the approach you would take to:\n",
        "\n",
        "    ‚óè Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "    ‚óè Choose and justify an appropriate model (SVM vs. Na√Øve Bayes)\n",
        "    ‚óè Address class imbalance\n",
        "    ‚óè Evaluate the performance of your solution with suitable metrics\n",
        "    \n",
        "And explain the business impact of your solution.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "zAEcWab__hnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans-**\n",
        "\n",
        "Approach\n",
        "\n",
        "1. Preprocessing\n",
        "\n",
        "- Handling Missing Data:\n",
        "\n",
        " - Drop emails with no text, or replace missing values with an empty string.\n",
        "\n",
        "- Text Vectorization:\n",
        "\n",
        " - Use TF-IDF Vectorizer (better than raw counts because it downweights common words like \"the\", \"and\").\n",
        "\n",
        "- Feature Engineering (optional):\n",
        "\n",
        " - Add metadata features (e.g., number of links, special characters, email length).\n",
        "\n",
        "2. Model Choice: SVM vs. Na√Øve Bayes\n",
        "\n",
        "- Na√Øve Bayes (MultinomialNB):\n",
        "\n",
        " - Works well for text classification, fast, interpretable.\n",
        "\n",
        " - Assumes word independence (na√Øve) but performs surprisingly well.\n",
        "\n",
        "- SVM:\n",
        "\n",
        " - Strong classifier with RBF/linear kernel, handles high-dimensional text data.\n",
        "\n",
        " - Slower for very large datasets.\n",
        "\n",
        "- Choice:\n",
        "\n",
        " - Start with Multinomial Na√Øve Bayes (fast, scalable).\n",
        "\n",
        " - Compare with Linear SVM to see if accuracy improves.\n",
        "\n",
        "3. Handling Class Imbalance\n",
        "\n",
        "- Options:\n",
        "\n",
        " - Use class weights (e.g., class_weight='balanced' in SVM).\n",
        "\n",
        " - Use resampling techniques (SMOTE oversampling spam, or undersampling ham).\n",
        "\n",
        " - Use threshold tuning on predicted probabilities.\n",
        "\n",
        "4. Evaluation Metrics\n",
        "\n",
        "- Since class imbalance exists, accuracy alone is misleading.\n",
        "Use:\n",
        "\n",
        " - Precision & Recall (especially recall for spam ‚Üí avoid missing spam emails).\n",
        "\n",
        " - F1-Score (balance between precision & recall).\n",
        "\n",
        " - ROC-AUC (overall discrimination ability).\n",
        "\n",
        "5. Business Impact\n",
        "\n",
        " - Reduced risk of missing spam ‚Üí protects users from phishing/fraud.\n",
        "\n",
        " - Less false positives ‚Üí ensures important legitimate emails aren‚Äôt marked spam.\n",
        "\n",
        " - Customer trust increases, fewer complaints about lost emails.\n",
        "\n",
        " - Operational efficiency ‚Üí automated filtering saves human effort."
      ],
      "metadata": {
        "id": "pD7MnmuWANKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spam Classification with Preprocessing & Evaluation\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load synthetic dataset (spam vs ham example)\n",
        "categories = ['sci.space', 'rec.sport.baseball']  # simulate ham vs spam\n",
        "data = fetch_20newsgroups(subset='all', categories=categories)\n",
        "\n",
        "X, y = data.data, data.target  # y=0/1\n",
        "\n",
        "# Simulate missing data\n",
        "X = [doc if doc.strip() != \"\" else \"missing\" for doc in X]\n",
        "\n",
        "# 2. Vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# 3. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 4a. Train Multinomial Na√Øve Bayes\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "y_pred_nb = nb_model.predict(X_test)\n",
        "y_probs_nb = nb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# 4b. Train Linear SVM with class balancing\n",
        "svm_model = LinearSVC(class_weight='balanced', random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "\n",
        "# 5. Evaluation\n",
        "print(\"=== Na√Øve Bayes Report ===\")\n",
        "print(classification_report(y_test, y_pred_nb, target_names=data.target_names))\n",
        "print(\"Na√Øve Bayes ROC-AUC:\", roc_auc_score(y_test, y_probs_nb))\n",
        "\n",
        "print(\"\\n=== Linear SVM Report ===\")\n",
        "print(classification_report(y_test, y_pred_svm, target_names=data.target_names))\n",
        "# LinearSVC does not provide probabilities directly ‚Üí skip ROC-AUC here\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGbAt10V_YlZ",
        "outputId": "733280a2-e83a-4ac4-ff0a-4be9e2534193"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Na√Øve Bayes Report ===\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "rec.sport.baseball       0.98      0.99      0.99       299\n",
            "         sci.space       0.99      0.98      0.99       296\n",
            "\n",
            "          accuracy                           0.99       595\n",
            "         macro avg       0.99      0.99      0.99       595\n",
            "      weighted avg       0.99      0.99      0.99       595\n",
            "\n",
            "Na√Øve Bayes ROC-AUC: 0.9997966193618367\n",
            "\n",
            "=== Linear SVM Report ===\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "rec.sport.baseball       0.99      0.99      0.99       299\n",
            "         sci.space       0.99      0.99      0.99       296\n",
            "\n",
            "          accuracy                           0.99       595\n",
            "         macro avg       0.99      0.99      0.99       595\n",
            "      weighted avg       0.99      0.99      0.99       595\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "udhyIASQCLfi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}